[NeMo W 2022-11-24 18:55:26 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.
[NeMo W 2022-11-24 18:55:28 experimental:27] Module <class 'nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers.IPATokenizer'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-11-24 18:55:28 experimental:27] Module <class 'nemo.collections.tts.models.radtts.RadTTSModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-11-24 18:55:28 nemo_logging:349] /Users/euna/opt/anaconda3/envs/NeMo-FastPitch/lib/python3.8/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
      ret = run_job(
    
[NeMo W 2022-11-24 18:55:28 nemo_logging:349] /Users/euna/opt/anaconda3/envs/NeMo-FastPitch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:712: UserWarning: You passed `Trainer(accelerator='cpu', precision=16)` but native AMP is not supported on CPU. Using `precision='bf16'` instead.
      rank_zero_warn(
    
[NeMo I 2022-11-24 18:55:28 exp_manager:315] Experiments will be logged at /Users/euna/Desktop/KU/WorkSpace/NeMo-FastPitch/ljspeech_to_6097_no_mixing_5_mins/FastPitch/2022-11-24_18-55-28
[NeMo I 2022-11-24 18:55:28 exp_manager:704] TensorboardLogger has been set up
[NeMo W 2022-11-24 18:55:28 exp_manager:971] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 1000. Please ensure that max_steps will run for at least 25 epochs to ensure that checkpointing will not error out.
[NeMo I 2022-11-24 18:55:31 tokenize_and_classify:87] Creating ClassifyFst grammars.
[NeMo W 2022-11-24 18:55:45 experimental:27] Module <class 'nemo_text_processing.g2p.modules.IPAG2P'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-11-24 18:55:45 modules:95] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.
[NeMo W 2022-11-24 18:55:46 nemo_logging:349] /Users/euna/opt/anaconda3/envs/NeMo-FastPitch/lib/python3.8/site-packages/torch/jit/annotations.py:299: UserWarning: TorchScript will treat type annotations of Tensor dtype-specific subtypes as if they are normal Tensors. dtype constraints are not enforced in compilation either.
      warnings.warn("TorchScript will treat type annotations of Tensor "
    
[NeMo I 2022-11-24 18:55:46 data:215] Loading dataset from /Users/euna/Desktop/KU/WorkSpace/NeMo-FastPitch/6097_manifest_train_dur_5_mins_local.json.
[NeMo I 2022-11-24 18:55:46 data:252] Loaded dataset with 114 files.
[NeMo I 2022-11-24 18:55:46 data:254] Dataset contains 0.08 hours.
[NeMo I 2022-11-24 18:55:46 data:356] Pruned 0 files. Final dataset contains 114 files
[NeMo I 2022-11-24 18:55:46 data:358] Pruned 0.00 hours. Final dataset contains 0.08 hours.
[NeMo W 2022-11-24 18:55:46 nemo_logging:349] /Users/euna/opt/anaconda3/envs/NeMo-FastPitch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(_create_warning_msg(
    
[NeMo I 2022-11-24 18:55:46 data:215] Loading dataset from /Users/euna/Desktop/KU/WorkSpace/NeMo-FastPitch/6097_manifest_dev_ns_all_local.json.
[NeMo I 2022-11-24 18:55:46 data:252] Loaded dataset with 2 files.
[NeMo I 2022-11-24 18:55:46 data:254] Dataset contains 0.00 hours.
[NeMo I 2022-11-24 18:55:46 data:356] Pruned 0 files. Final dataset contains 2 files
[NeMo I 2022-11-24 18:55:46 data:358] Pruned 0.00 hours. Final dataset contains 0.00 hours.
[NeMo I 2022-11-24 18:55:46 features:225] PADDING: 1
[NeMo I 2022-11-24 18:55:48 tokenize_and_classify:87] Creating ClassifyFst grammars.
[NeMo W 2022-11-24 18:56:02 modules:95] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.
[NeMo W 2022-11-24 18:56:02 modelPT:142] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.
    Train config : 
    dataset:
      _target_: nemo.collections.tts.torch.data.TTSDataset
      manifest_filepath: /ws/LJSpeech/nvidia_ljspeech_train_clean_ngc.json
      sample_rate: 22050
      sup_data_path: /raid/LJSpeech/supplementary
      sup_data_types:
      - align_prior_matrix
      - pitch
      n_fft: 1024
      win_length: 1024
      hop_length: 256
      window: hann
      n_mels: 80
      lowfreq: 0
      highfreq: 8000
      max_duration: null
      min_duration: 0.1
      ignore_file: null
      trim: false
      pitch_fmin: 65.40639132514966
      pitch_fmax: 2093.004522404789
      pitch_norm: true
      pitch_mean: 212.35873413085938
      pitch_std: 68.52806091308594
      use_beta_binomial_interpolator: true
    dataloader_params:
      drop_last: false
      shuffle: true
      batch_size: 24
      num_workers: 0
    
[NeMo W 2022-11-24 18:56:02 modelPT:149] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). 
    Validation config : 
    dataset:
      _target_: nemo.collections.tts.torch.data.TTSDataset
      manifest_filepath: /ws/LJSpeech/nvidia_ljspeech_val_clean_ngc.json
      sample_rate: 22050
      sup_data_path: /raid/LJSpeech/supplementary
      sup_data_types:
      - align_prior_matrix
      - pitch
      n_fft: 1024
      win_length: 1024
      hop_length: 256
      window: hann
      n_mels: 80
      lowfreq: 0
      highfreq: 8000
      max_duration: null
      min_duration: null
      ignore_file: null
      trim: false
      pitch_fmin: 65.40639132514966
      pitch_fmax: 2093.004522404789
      pitch_norm: true
      pitch_mean: 212.35873413085938
      pitch_std: 68.52806091308594
      use_beta_binomial_interpolator: true
    dataloader_params:
      drop_last: false
      shuffle: false
      batch_size: 24
      num_workers: 0
    
[NeMo I 2022-11-24 18:56:02 features:225] PADDING: 1
[NeMo I 2022-11-24 18:56:02 save_restore_connector:243] Model FastPitchModel was successfully restored from /Users/euna/Desktop/KU/WorkSpace/NeMo-FastPitch/tts_en_fastpitch_align.nemo.
[NeMo I 2022-11-24 18:56:02 modelPT:1067] Model checkpoint restored from nemo file with path : `/Users/euna/Desktop/KU/WorkSpace/NeMo-FastPitch/tts_en_fastpitch_align.nemo`
[NeMo W 2022-11-24 18:56:02 nemo_logging:349] /Users/euna/opt/anaconda3/envs/NeMo-FastPitch/lib/python3.8/site-packages/pytorch_lightning/callbacks/base.py:22: LightningDeprecationWarning: pytorch_lightning.callbacks.base.Callback has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.callbacks.callback.Callback class instead.
      rank_zero_deprecation(
    
[NeMo I 2022-11-24 18:56:02 modelPT:602] Optimizer config = Adam (
    Parameter Group 0
        amsgrad: False
        betas: [0.9, 0.999]
        capturable: False
        differentiable: False
        eps: 1e-08
        foreach: None
        fused: False
        lr: 0.0002
        maximize: False
        weight_decay: 1e-06
    )
[NeMo I 2022-11-24 18:56:02 lr_scheduler:772] Scheduler not initialized as no `sched` config supplied to setup_optimizer()
